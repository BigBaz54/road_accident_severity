{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management import *\n",
    "from src.plotting import *\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    # DecisionTreeClassifier(), \n",
    "    RandomForestClassifier(n_jobs=-1), # OK\n",
    "    # BaggingClassifier(n_jobs=-1), \n",
    "    LogisticRegression(n_jobs=-1), # SUPER OK\n",
    "    # SVC(gamma='auto', C=1, cache_size=1900), # LONG (30k : 4min, 10k : 30s, 5k : 4s)\n",
    "    GaussianNB(), # SUPER OK\n",
    "    # SGDClassifier(n_jobs=-1),\n",
    "    KNeighborsClassifier(n_jobs=-1), # SUPER OK\n",
    "    #GradientBoostingClassifier(),\n",
    "    MLPClassifier(), # OK\n",
    "    #AdaBoostClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "Data is loaded from the files and joined. Each feature is then processed to ensure that it is in the correct format for the model and then some features are dropped (according to the feature selection process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pca = 10\n",
    "data_without_grav, grav, data_PCA = workable_data(nb_pca)\n",
    "\n",
    "nb_lines = 1000\n",
    "data_without_grav = data_without_grav[:nb_lines]\n",
    "grav = grav[:nb_lines]\n",
    "data_PCA = data_PCA[:nb_lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_without_grav` contains the 26 selected features, `grav` is the target variable and `data_PCA` contains the 5 principal components of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the list of hyperparameters\n",
    "To simplify the hyperparameter search, we use the `get_params` method of the classifier to get the list of hyperparameters that can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: RandomForestClassifier\n",
      "Parameters:\n",
      "\t bootstrap\n",
      "\t ccp_alpha\n",
      "\t class_weight\n",
      "\t criterion\n",
      "\t max_depth\n",
      "\t max_features\n",
      "\t max_leaf_nodes\n",
      "\t max_samples\n",
      "\t min_impurity_decrease\n",
      "\t min_samples_leaf\n",
      "\t min_samples_split\n",
      "\t min_weight_fraction_leaf\n",
      "\t n_estimators\n",
      "\t n_jobs\n",
      "\t oob_score\n",
      "\t random_state\n",
      "\t verbose\n",
      "\t warm_start\n",
      "\n",
      "Classifier: LogisticRegression\n",
      "Parameters:\n",
      "\t C\n",
      "\t class_weight\n",
      "\t dual\n",
      "\t fit_intercept\n",
      "\t intercept_scaling\n",
      "\t l1_ratio\n",
      "\t max_iter\n",
      "\t multi_class\n",
      "\t n_jobs\n",
      "\t penalty\n",
      "\t random_state\n",
      "\t solver\n",
      "\t tol\n",
      "\t verbose\n",
      "\t warm_start\n",
      "\n",
      "Classifier: GaussianNB\n",
      "Parameters:\n",
      "\t priors\n",
      "\t var_smoothing\n",
      "\n",
      "Classifier: KNeighborsClassifier\n",
      "Parameters:\n",
      "\t algorithm\n",
      "\t leaf_size\n",
      "\t metric\n",
      "\t metric_params\n",
      "\t n_jobs\n",
      "\t n_neighbors\n",
      "\t p\n",
      "\t weights\n",
      "\n",
      "Classifier: MLPClassifier\n",
      "Parameters:\n",
      "\t activation\n",
      "\t alpha\n",
      "\t batch_size\n",
      "\t beta_1\n",
      "\t beta_2\n",
      "\t early_stopping\n",
      "\t epsilon\n",
      "\t hidden_layer_sizes\n",
      "\t learning_rate\n",
      "\t learning_rate_init\n",
      "\t max_fun\n",
      "\t max_iter\n",
      "\t momentum\n",
      "\t n_iter_no_change\n",
      "\t nesterovs_momentum\n",
      "\t power_t\n",
      "\t random_state\n",
      "\t shuffle\n",
      "\t solver\n",
      "\t tol\n",
      "\t validation_fraction\n",
      "\t verbose\n",
      "\t warm_start\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Parameters:\")\n",
    "    for key in classifier.get_params():\n",
    "        print(\"\\t\", key)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing the hyperparameters to tune\n",
    "We then need to choose from the list above which hyperparameters we want to tune. We can also choose the range of values to test for each hyperparameter.\n",
    "\n",
    "The `param_grid` variable is a dictionary where the keys are the names of the hyperparameters and the values are the list of values to test for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = []\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"max_depth\": [None, 5, 10, 20, 50, 100],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "if \"DecisionTreeClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# RandomForestClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "if \"RandomForestClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# BaggingClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 20, 50, 100],\n",
    "    \"max_samples\": [0.1, 0.5, 1.0],\n",
    "    \"max_features\": [0.1, 0.5, 1.0],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"bootstrap_features\": [True, False]\n",
    "}\n",
    "if \"BaggingClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# LogisticRegression\n",
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"C\": [0.1, 0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "    \"max_iter\": [100, 200, 500]\n",
    "}\n",
    "if \"LogisticRegression\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# SVC\n",
    "param_grid = {\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"C\": [0.1, 0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "if \"SVC\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# GaussianNB\n",
    "param_grid = {\n",
    "    \"var_smoothing\": [1e-07, 1e-06, 1e-05, 1e-04, 1e-03, 0.005, 0.01, 0.02, 0.05, 0.075, 0.1]\n",
    "}\n",
    "if \"GaussianNB\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# SGDClassifier\n",
    "param_grid = {\n",
    "    \"loss\": [\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"alpha\": [0.00001, 0.0001, 0.001, 0.01],\n",
    "    \"max_iter\": [1000, 2000, 5000, 10000],\n",
    "}\n",
    "if \"SGDClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# KNeighborsClassifier\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [1, 2, 5, 10],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": [1, 2, 5, 10, 20, 30, 50],\n",
    "    \"p\": [1, 2]\n",
    "}\n",
    "if \"KNeighborsClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.005, 0.01, 0.025, 0.05, 0.1, 0.5],\n",
    "    \"n_estimators\": [100, 500], \n",
    "    \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
    "    \"max_depth\": [1, 2, 3, 5, 10],\n",
    "    \"min_samples_split\": [2, 5, 10, 15, 20],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "if \"GradientBoostingClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# MLPClassifier\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\":  [(20,20,), (100,), (200,), (500,)],\n",
    "    \"activation\": [\"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\": [\"lbfgs\", \"sgd\"],\n",
    "    \"alpha\": [0.00001, 0.0001, 0.001, 0.01] ,\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "    \"max_iter\": [200, 500]\n",
    "}\n",
    "if \"MLPClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# AdaBoostClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 200, 500],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 0.5],\n",
    "    \"algorithm\": [\"SAMME\", \"SAMME.R\"]\n",
    "}\n",
    "if \"AdaBoostClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the models with all combinations of hyperparameters\n",
    "We use the `GridSearchCV` class to fit the models with all combinations of hyperparameters and find the best hyperparameters for each model.\n",
    "\n",
    "This class uses cross-validation to evaluate the performance through an exhaustive search over the hyperparameter values space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: RandomForestClassifier\n",
      "Parameters:\n",
      "\tn_estimators: [10, 50, 100, 200, 500]\n",
      "\tcriterion   : ['gini', 'entropy']\n",
      "\tmax_depth   : [None, 10, 50, 200]\n",
      "\tmin_samples_split: [10, 50, 200]\n",
      "\tmin_samples_leaf: [5, 20, 50, 200]\n",
      "\tmax_features: ['sqrt', 'log2']\n",
      "\n",
      "Best parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Best score: 0.610\n",
      "\n",
      "############################################\n",
      "\n",
      "Classifier: LogisticRegression\n",
      "Parameters:\n",
      "\tpenalty     : ['l1', 'l2', 'elasticnet']\n",
      "\tC           : [0.1, 0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
      "\tsolver      : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
      "\tmax_iter    : [100, 200, 500]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "792 fits failed out of a total of 1485.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1179, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5489951         nan 0.55698513 0.55299311\n",
      " 0.55000509 0.5520011  0.55199511 0.5449851         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.5489951\n",
      "        nan 0.55399112 0.55299311 0.54000108 0.5520011  0.54299509\n",
      " 0.55099411        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.5489951         nan 0.55098512 0.55299311\n",
      " 0.54300408 0.5520011  0.54400209 0.54500009        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.55400011\n",
      "        nan 0.54898911 0.54400808 0.54299509 0.53900907 0.55299911\n",
      " 0.5449881         nan        nan        nan        nan        nan\n",
      "        nan        nan 0.55400011        nan 0.54999011 0.54400808\n",
      " 0.53500207 0.53900907 0.54399909 0.5509971         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.55400011\n",
      "        nan 0.5489981  0.54400808 0.53700707 0.53900907 0.53700707\n",
      " 0.54399609        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54400508        nan 0.5439871  0.53901207\n",
      " 0.53002104 0.53600607 0.5509971  0.5439901         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54400508\n",
      "        nan 0.55299611 0.53901207 0.53700707 0.53600607 0.54499709\n",
      " 0.55199811        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54400508        nan 0.54399609 0.53901207\n",
      " 0.53002104 0.53600607 0.53800807 0.54300108        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54501208\n",
      "        nan 0.5439871  0.54101407 0.53401305 0.54101706 0.55299911\n",
      " 0.5449881         nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54601008        nan 0.5509971  0.54101407\n",
      " 0.53101605 0.54101706 0.54499709 0.55199811        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54501208\n",
      "        nan 0.54200008 0.54101407 0.52801904 0.54101706 0.53700707\n",
      " 0.54300408        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54400808        nan 0.5449881  0.54001606\n",
      " 0.54300108 0.54101706 0.5510001  0.5449881         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54500908\n",
      "        nan 0.55199811 0.54001606 0.54001007 0.54101706 0.54499709\n",
      " 0.55199811        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54400808        nan 0.54100208 0.54001606\n",
      " 0.53201405 0.54101706 0.53700707 0.54300408        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54401407\n",
      "        nan 0.5449881  0.53901506 0.53801106 0.54101107 0.5499991\n",
      " 0.5449881         nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.55199811 0.53901506\n",
      " 0.53400706 0.54101107 0.54499709 0.55199811        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54401407\n",
      "        nan 0.54200308 0.53901506 0.53101904 0.54101107 0.53700707\n",
      " 0.54300408        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.5449881  0.53801406\n",
      " 0.54400808 0.54501208 0.55199811 0.5449881         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54501208\n",
      "        nan 0.55199811 0.53801406 0.53100705 0.54501208 0.54499709\n",
      " 0.55199811        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.54300408 0.53801406\n",
      " 0.52902004 0.54501208 0.53700707 0.54200608        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54501208\n",
      "        nan 0.5449881  0.53901506 0.53400706 0.54501208 0.55299911\n",
      " 0.5449881         nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.55199811 0.53901506\n",
      " 0.53300906 0.54501208 0.54499709 0.55199811        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54401407\n",
      "        nan 0.54200608 0.53901506 0.53900907 0.54501208 0.53700707\n",
      " 0.54200608        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.5449881  0.53901506\n",
      " 0.54200308 0.54501208 0.5510001  0.5449881         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54501208\n",
      "        nan 0.55199811 0.53901506 0.53000605 0.54501208 0.54499709\n",
      " 0.55199811        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.54200608 0.53901506\n",
      " 0.53400406 0.54501208 0.53700707 0.54200608        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54501208\n",
      "        nan 0.54598611 0.53901506 0.54200907 0.54601308 0.5499991\n",
      " 0.5449881         nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.55199811 0.53901506\n",
      " 0.53900307 0.54601308 0.54499709 0.5510001         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54501208\n",
      "        nan 0.54200608 0.53901506 0.52502203 0.54601308 0.53700707\n",
      " 0.54200608        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.5449881  0.53901506\n",
      " 0.54399309 0.54601308 0.55199811 0.5449881         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.54501208\n",
      "        nan 0.55199811 0.53901506 0.52901404 0.54601308 0.54499709\n",
      " 0.5510001         nan        nan        nan        nan        nan\n",
      "        nan        nan 0.54501208        nan 0.54200608 0.53901506\n",
      " 0.52701204 0.54601308 0.53700707 0.54200608        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best score: 0.557\n",
      "\n",
      "############################################\n",
      "\n",
      "Classifier: GaussianNB\n",
      "Parameters:\n",
      "\tvar_smoothing: [1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.005, 0.01, 0.02, 0.05, 0.075, 0.1]\n",
      "\n",
      "Best parameters: {'var_smoothing': 0.02}\n",
      "Best score: 0.517\n",
      "\n",
      "############################################\n",
      "\n",
      "Classifier: KNeighborsClassifier\n",
      "Parameters:\n",
      "\tn_neighbors : [1, 2, 5, 10]\n",
      "\tweights     : ['uniform', 'distance']\n",
      "\talgorithm   : ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
      "\tleaf_size   : [1, 2, 5, 10, 20, 30, 50]\n",
      "\tp           : [1, 2]\n",
      "\n",
      "Best parameters: {'algorithm': 'ball_tree', 'leaf_size': 10, 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "Best score: 0.535\n",
      "\n",
      "############################################\n",
      "\n",
      "Classifier: MLPClassifier\n",
      "Parameters:\n",
      "\thidden_layer_sizes: [(20, 20), (100,), (200,), (500,)]\n",
      "\tactivation  : ['logistic', 'tanh', 'relu']\n",
      "\tsolver      : ['lbfgs', 'sgd']\n",
      "\talpha       : [1e-05, 0.0001, 0.001, 0.01]\n",
      "\tlearning_rate: ['constant', 'adaptive']\n",
      "\tmax_iter    : [200, 500]\n",
      "\n",
      "Best parameters: {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "Best score: 0.578\n",
      "\n",
      "############################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_params = []\n",
    "best_scores = []\n",
    "\n",
    "for classifier, param_grid in zip(classifiers, param_grids):\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Parameters:\")\n",
    "    for key in param_grid:\n",
    "        print(f\"\\t{key:12}: {param_grid[key]}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=3, verbose=0, n_jobs=-1)\n",
    "    grid_search.fit(data_without_grav, grav)\n",
    "    best_params.append(grid_search.best_params_)\n",
    "    best_scores.append(grid_search.best_score_)\n",
    "    print(\"Best parameters:\", best_params[-1])\n",
    "    print(f\"Best score: {best_scores[-1]:.3f}\")\n",
    "    print(\"\\n############################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: RandomForestClassifier\n",
      "Best parameters:\n",
      "\tcriterion   : gini\n",
      "\tmax_depth   : None\n",
      "\tmax_features: sqrt\n",
      "\tmin_samples_leaf: 5\n",
      "\tmin_samples_split: 10\n",
      "\tn_estimators: 50\n",
      "Best score: 0.610\n",
      "\n",
      "############################################\n",
      "\n",
      "Classifier: LogisticRegression\n",
      "Best parameters:\n",
      "\tC           : 0.1\n",
      "\tmax_iter    : 100\n",
      "\tpenalty     : l1\n",
      "\tsolver      : saga\n",
      "Best score: 0.557\n",
      "\n",
      "############################################\n",
      "\n",
      "Classifier: GaussianNB\n",
      "Best parameters:\n",
      "\tvar_smoothing: 0.02\n",
      "Best score: 0.517\n",
      "\n",
      "############################################\n",
      "\n",
      "Classifier: KNeighborsClassifier\n",
      "Best parameters:\n",
      "\talgorithm   : ball_tree\n",
      "\tleaf_size   : 10\n",
      "\tn_neighbors : 10\n",
      "\tp           : 1\n",
      "\tweights     : distance\n",
      "Best score: 0.535\n",
      "\n",
      "############################################\n",
      "\n",
      "Classifier: MLPClassifier\n",
      "Best parameters:\n",
      "\tactivation  : logistic\n",
      "\talpha       : 0.001\n",
      "\thidden_layer_sizes: (100,)\n",
      "\tlearning_rate: constant\n",
      "\tmax_iter    : 500\n",
      "\tsolver      : sgd\n",
      "Best score: 0.578\n",
      "\n",
      "############################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier, best_param, best_scores in zip(classifiers, best_params, best_scores):\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Best parameters:\")\n",
    "    for key in best_param:\n",
    "        print(f\"\\t{key:12}: {best_param[key]}\")\n",
    "    print(f\"Best score: {best_scores:.3f}\")\n",
    "    print(\"\\n############################################\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
